{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed1d2d75",
      "metadata": {
        "id": "ed1d2d75"
      },
      "source": [
        "# CS 342 Homework 2 - Convolutional Neural Networks\n",
        "\n",
        "Welcome to your second homework for CS 342! This problem set covers Convolutional Neural Networks. There are three main problems with several sub-questions each. For coding questions, fill in the missing parts (usually denoted `...`). For theoretical questions, write the most succinct possible answer in the provided markdown block. Please answer all questions in-line, being as brief and precise as possible. You will not need any libraries that aren't already imported here. This code can be run on your local machine.\n",
        "\n",
        "Please follow <a href=\"https://pytorch.org\">these instructions</a> to install pytorch.\n",
        "\n",
        "Submission: Upload your jupyter notebook on canvas.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c154dc03",
      "metadata": {
        "id": "c154dc03"
      },
      "source": [
        "Enter your name and EID in the following block:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a17cf8c2",
      "metadata": {
        "id": "a17cf8c2"
      },
      "source": [
        "Hyunsung Oh (ho3626)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8948051e",
      "metadata": {
        "id": "8948051e"
      },
      "source": [
        "If you worked with anyone on this homework, please list them below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06a1bfb5",
      "metadata": {
        "id": "06a1bfb5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "56a77e25",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T05:46:08.876471Z",
          "start_time": "2022-02-23T05:46:08.367373Z"
        },
        "id": "56a77e25"
      },
      "outputs": [],
      "source": [
        "# Run these two blocks to load important libraries and set things up\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3f184e87",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T05:46:08.923145Z",
          "start_time": "2022-02-23T05:46:08.915511Z"
        },
        "id": "3f184e87"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33930ea1",
      "metadata": {
        "id": "33930ea1"
      },
      "source": [
        "## Problem 1. Training CNNs on the Fashion MNIST dataset (22 pts)\n",
        "\n",
        "In this problem, we will build CNNs on a freely available vision dataset, <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST</a>. We have explored this dataset with an MLP architecture and will now try CNNs.\n",
        "\n",
        "You might recollect that Fashion MNIST consists of many 28x28 grayscale images belonging to 10 different classes of clothing. The task is to train a classifier that can predict the clothing class from the image.\n",
        "\n",
        "Since several of the steps follow from your work on HW1, you can fill in the misisng pieces much like last time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "29d3c71b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-18T05:01:45.091340Z",
          "start_time": "2022-02-18T05:01:45.083102Z"
        },
        "id": "29d3c71b"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "01b76859",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-18T05:01:54.235278Z",
          "start_time": "2022-02-18T05:01:53.872351Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01b76859",
        "outputId": "fd1aeaad-5ea8-4991-b924-41741411bfe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 19.1MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 304kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.57MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 10.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: fashionMNIST_data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: fashionMNIST_data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "save_dir = 'fashionMNIST_data'\n",
        "\n",
        "transform = transforms.ToTensor() # Convert the image into a torch tensor.\n",
        "\n",
        "train_set = datasets.FashionMNIST(save_dir, download=True, train=True, transform=transform)\n",
        "test_set = datasets.FashionMNIST(save_dir, download=True, train=False, transform=transform)\n",
        "\n",
        "print(train_set)\n",
        "print(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd6e8db",
      "metadata": {
        "id": "6bd6e8db"
      },
      "source": [
        "Each of these sets comprises of the image `data` and the clasification `targets`. The `targets` take a numerical value from 1-10 indicating which clothing class each image belongs to."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62a68c1",
      "metadata": {
        "id": "a62a68c1"
      },
      "source": [
        "Since the original data does not have a validation set, let's (once again) create one by splitting the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "75073bc5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-18T05:02:35.429323Z",
          "start_time": "2022-02-18T05:02:35.401409Z"
        },
        "id": "75073bc5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "ntotal = 60000\n",
        "ntrain = int(0.9*ntotal)\n",
        "nval = ntotal - ntrain\n",
        "\n",
        "val_ix = np.random.choice(range(ntotal), size=nval, replace=False)\n",
        "train_ix = list(set(range(ntotal)) - set(val_ix))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_ix)\n",
        "val_sampler = SubsetRandomSampler(val_ix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2765b68",
      "metadata": {
        "id": "d2765b68"
      },
      "source": [
        "### Q 1.1: Initialize the train, val and test dataloaders with the given `batch_size` (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0c3b5bc4",
      "metadata": {
        "id": "0c3b5bc4"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=val_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad20723",
      "metadata": {
        "id": "9ad20723"
      },
      "source": [
        "Each loader iterates over the data, yielding `batch_size` images and output targets per iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c3c84f",
      "metadata": {
        "id": "57c3c84f"
      },
      "source": [
        "### Q 1.2: Initialize a CNN with 2-D convolution layer(s), max pooling and finally, fully-connected layer(s). You can add any non-linearity of your choice and use >=1 layer of each type overall. (4 pts)\n",
        "\n",
        "We have provided the skeleton class definition below. Complete it by specifying the layer objects in your CNN. Keep in mind that the input `x` is of shape (`batch_size`, 1, 28, 28). Unlike the MLP, you do not have to flatten it to feed it into the model. Additionally, the output should be of shape (`batch_size`, 10) as we have 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b52de04e",
      "metadata": {
        "id": "b52de04e"
      },
      "outputs": [],
      "source": [
        "class fashionMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(fashionMLP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))  # (batch_size, 32, 28, 28)\n",
        "        x = self.pool(x)               # (batch_size, 32, 14, 14)\n",
        "        x = torch.relu(self.conv2(x))  # (batch_size, 64, 14, 14)\n",
        "        x = self.pool(x)               # (batch_size, 64, 7, 7)\n",
        "        x = x.view(x.size(0), -1)      # flatten: (batch_size, 64*7*7)\n",
        "        x = torch.relu(self.fc1(x))    # (batch_size, 128)\n",
        "        x = self.fc2(x)                # (batch_size, 10)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a477ba00",
      "metadata": {
        "id": "a477ba00"
      },
      "source": [
        "Tip: An easy way to check if you got the dimensions right is to test your model on a single batch. Iteratively add each layer transformation to the input, and ensure the shape is right at each stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8fa59a0c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-18T05:29:00.097342Z",
          "start_time": "2022-02-18T05:29:00.095068Z"
        },
        "id": "8fa59a0c"
      },
      "outputs": [],
      "source": [
        "# for images, labels in train_loader:\n",
        "#     print(images.shape, labels.shape)\n",
        "#     break\n",
        "# model = fashionMLP()\n",
        "# outputs = model(...)\n",
        "# print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1472d110",
      "metadata": {
        "id": "1472d110"
      },
      "source": [
        "### Q 1.3: Define `criterion` to be the cross entropy loss function and use an optimizer of your choice (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a155b927",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-18T05:29:01.148897Z",
          "start_time": "2022-02-18T05:29:01.145236Z"
        },
        "id": "a155b927"
      },
      "outputs": [],
      "source": [
        "model = fashionMLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cfadf71",
      "metadata": {
        "id": "6cfadf71"
      },
      "source": [
        "### Q 1.4: Complete the training, validation and testing loops (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "67eb9bc5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-18T05:29:04.258161Z",
          "start_time": "2022-02-18T05:29:04.251130Z"
        },
        "id": "67eb9bc5"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_network(model, train_loader, val_loader, criterion, optimizer, nepoch=100):\n",
        "    try:\n",
        "        for epoch in tqdm(range(nepoch)):\n",
        "            print('EPOCH %d'%epoch)\n",
        "            total_loss = 0\n",
        "            count = 0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                count += 1\n",
        "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
        "            with torch.no_grad():\n",
        "                total_loss = 0\n",
        "                count = 0\n",
        "                for inputs, labels in val_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    total_loss += loss.item()\n",
        "                    count += 1\n",
        "                print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n",
        "            print()\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting from training early')\n",
        "    return\n",
        "\n",
        "def test_network(model, test_loader, mode):\n",
        "    true, pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels  in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "            labels = labels.cpu().numpy()\n",
        "            predicted = predicted.cpu().numpy()\n",
        "            true.append(labels)\n",
        "            pred.append(predicted)\n",
        "    acc = (np.concatenate(true) == np.concatenate(pred)).mean()\n",
        "    print('%s accuracy: %0.3f' % (mode, acc))\n",
        "    true = np.concatenate(true)\n",
        "    pred = np.concatenate(pred)\n",
        "    return acc, true, pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad0e136",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-01T16:21:58.510967Z",
          "start_time": "2022-02-01T16:21:58.505264Z"
        },
        "id": "3ad0e136"
      },
      "source": [
        "### Q 1.5: Train the network and report the final test accuracy of your model (2 pts)\n",
        "Use the functions `train_network` and `test_network` defined above.\n",
        "\n",
        "Model test accuracy should be 86% or greater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "486e6455",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-18T05:29:06.005651Z",
          "start_time": "2022-02-18T05:29:06.000862Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "8e769976b4b84f7f8a97894d7cc3657d",
            "8e42811b0fe84da08b7857da66660cd9",
            "aaa6247976314d56b90fcdd602d5d92f",
            "fc1a191696054d6982db7026bfbc64b4",
            "5a03fc8fe49c40cf995b3f0293573e4e",
            "b7bea91d05f8419eacfb411f1fff1190",
            "9cab7259f6a549d3870f5918b9a5dbf3",
            "4edc42089643408280a6f7a752e07dcf",
            "ed6732a7917d452195e770407e636d74",
            "c9b86d8cab564bd9b3de1239370e6b2f",
            "b760c414aa554eaab3d43c0727088b50"
          ]
        },
        "id": "486e6455",
        "outputId": "bc244568-eb45-467c-8419-d6c2e1eeb988"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e769976b4b84f7f8a97894d7cc3657d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0\n",
            "Exiting from training early\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-015e1442db50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Test Accuracy: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a46a90654e6c>\u001b[0m in \u001b[0;36mtest_network\u001b[0;34m(model, test_loader, mode)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9d65e831e578>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, 32, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (batch_size, 32, 14, 14)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, 64, 14, 14)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (batch_size, 64, 7, 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         return F.max_pool2d(\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = fashionMLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_network(model, train_loader, val_loader, criterion, optimizer, nepoch=20)\n",
        "\n",
        "test_acc, true, pred = test_network(model, test_loader, mode=\"Test\")\n",
        "print(\"Final Test Accuracy: {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7feb0a",
      "metadata": {
        "id": "ef7feb0a"
      },
      "source": [
        "### Q 1.6: Which class did the model get wrong the most? Which class did it get right the most? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea29259c",
      "metadata": {
        "id": "ea29259c"
      },
      "source": [
        "Shirt class got wrong the most and Trouser class got right the most."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40367ae",
      "metadata": {
        "id": "d40367ae"
      },
      "source": [
        "### Q 1.7: Add any regularization method of your choice (or more than one!), and report the final test accuracy (4 pts)\n",
        " You should be able to get accuracy > 90%.\n",
        "\n",
        " The model accuracy for this should be greater than the test accuracy calculated in Q1.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bbbaee72",
      "metadata": {
        "id": "bbbaee72"
      },
      "outputs": [],
      "source": [
        "class regularized_fashionMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(32)\n",
        "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(128)\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.dropout_fc = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "47f40e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d1c7290d23e6462eb4a63fefbe2b15b6",
            "89d10523ce6547d4bcc6b92f05fd6f30",
            "511651f9142344279ad45b72a951cd76",
            "a09d5601f7184bb9adcb3632f2482c19",
            "5ac512bb923b477e9b442b77586d9de9",
            "4a96b9ada84949758fa13f7bccf5d9a8",
            "422e407e37fd433b8d4cd2dec5b431c8",
            "eeea291271484fd38b61a860789e91f4",
            "66a7648bf8e84675a1f8216864424672",
            "e203eaa3b71c463fa0ca2e11ba5705a9",
            "8eba5cba374846258d250c9b6739b4f1"
          ]
        },
        "id": "47f40e7b",
        "outputId": "3bae0874-ca91-4dc1-81d4-3a276875f054"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1c7290d23e6462eb4a63fefbe2b15b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0\n",
            " Train loss: 0.65876\n",
            "   Val loss: 0.47469\n",
            "\n",
            "EPOCH 1\n",
            " Train loss: 0.40772\n",
            "   Val loss: 0.38102\n",
            "\n",
            "EPOCH 2\n",
            " Train loss: 0.35259\n",
            "   Val loss: 0.33734\n",
            "\n",
            "EPOCH 3\n",
            " Train loss: 0.32480\n",
            "   Val loss: 0.31287\n",
            "\n",
            "EPOCH 4\n",
            " Train loss: 0.30556\n",
            "   Val loss: 0.30383\n",
            "\n",
            "EPOCH 5\n",
            " Train loss: 0.29166\n",
            "   Val loss: 0.31064\n",
            "\n",
            "EPOCH 6\n",
            " Train loss: 0.27820\n",
            "   Val loss: 0.29095\n",
            "\n",
            "EPOCH 7\n",
            " Train loss: 0.26742\n",
            "   Val loss: 0.28167\n",
            "\n",
            "EPOCH 8\n",
            " Train loss: 0.25804\n",
            "   Val loss: 0.27607\n",
            "\n",
            "EPOCH 9\n",
            " Train loss: 0.25050\n",
            "   Val loss: 0.32486\n",
            "\n",
            "EPOCH 10\n",
            " Train loss: 0.24361\n",
            "   Val loss: 0.27043\n",
            "\n",
            "EPOCH 11\n",
            " Train loss: 0.23682\n",
            "   Val loss: 0.26521\n",
            "\n",
            "EPOCH 12\n",
            " Train loss: 0.23277\n",
            "   Val loss: 0.27370\n",
            "\n",
            "EPOCH 13\n",
            " Train loss: 0.22492\n",
            "   Val loss: 0.26752\n",
            "\n",
            "EPOCH 14\n",
            " Train loss: 0.22013\n",
            "   Val loss: 0.28049\n",
            "\n",
            "EPOCH 15\n",
            " Train loss: 0.21612\n",
            "   Val loss: 0.27534\n",
            "\n",
            "EPOCH 16\n",
            " Train loss: 0.21037\n",
            "   Val loss: 0.26748\n",
            "\n",
            "EPOCH 17\n",
            " Train loss: 0.20608\n",
            "   Val loss: 0.26559\n",
            "\n",
            "EPOCH 18\n",
            " Train loss: 0.20438\n",
            "   Val loss: 0.25294\n",
            "\n",
            "EPOCH 19\n",
            " Train loss: 0.19985\n",
            "   Val loss: 0.26049\n",
            "\n",
            "Test (Regularized) accuracy: 0.910\n",
            "Final Test Accuracy (Regularized Model): 0.910\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model, criterion, and optimizer\n",
        "regmodel = regularized_fashionMLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(regmodel.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Train the model\n",
        "train_network(regmodel, train_loader, val_loader, criterion, optimizer, nepoch=20)\n",
        "\n",
        "regmodel.eval() # disables some regularization methods for model testing\n",
        "\n",
        "# Report test accuracy\n",
        "test_acc, true, pred = test_network(regmodel, test_loader, mode=\"Test (Regularized)\")\n",
        "print(\"Final Test Accuracy (Regularized Model): {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f058b20",
      "metadata": {
        "id": "2f058b20"
      },
      "source": [
        "### Q 1.8: Visualize learned kernels from first layer of your CNN (2 pts)\n",
        "\n",
        "You can either plot them separately (i.e. each as one panel in a plot) or stack them together into a single array. In either case, use matplotlib `imshow` or `matshow` to make an image of each kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3680e5f0",
      "metadata": {
        "id": "3680e5f0"
      },
      "outputs": [],
      "source": [
        "# first, extract the weights from the learned model\n",
        "conv1wt = model.conv1.weight.data.clone().cpu().numpy()\n",
        "\n",
        "# then, visualize them!\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "n_kernels = conv1wt.shape[0]\n",
        "n_cols = 8\n",
        "n_rows = math.ceil(n_kernels / n_cols)\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*1.5, n_rows*1.5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(n_kernels):\n",
        "    kernel = conv1wt[i, 0, :, :]\n",
        "    axes[i].imshow(kernel, cmap='gray')\n",
        "    axes[i].set_title(f\"Kernel {i}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "for j in range(n_kernels, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e120bbbc",
      "metadata": {
        "id": "e120bbbc"
      },
      "source": [
        "## Problem 2. Training 1-D CNNs for phoneme classification (34 pts)\n",
        "\n",
        "In this problem, we will build CNNs to classify phonemes from speech features. Instead of using the raw audio waveform as the input to a neural network, we can extract relevant features from the power spectrum of the waveform. Since this is not relevant for our exercise, we have precomputed speech features for you on the input waveform. The data comprises different episodes from a podcast _The Moth Radio Hour_ wherein a single speaker narrates a personal story. You can find it in `HW2_phone_class_data`.\n",
        "<br><br>\n",
        "Speech Features: These are 240-D features extracted for every 10ms snippet in a story.\n",
        "<br>\n",
        "Phoneme labels: We give you the raw labels and the associated start and end time of the phoneme.\n",
        "<br><br>\n",
        "Note that the speech features were extracted at a much higher sampling rate than the typical frequency of a phoneme. Consequeently, there are several speech features occuring between the start and end time of a single phoneme. To predict a single phoneme label, we will firstly consider a `window_size` of 10. Next, we will pick the 10 speech features that occurred right before the phoneme ended. For example, consider a hypothetical scenario where a phoneme started at 1:00 min and ended at 3:00 min, and the speech features were collected every 10s. Then, we would extract features at `[3:00 min, 2:50 min, 2:40 min, 2:30 min, 2:20 min, 2:10 min, 2:00 min, 1:50 min, 1:40 min and 1:30 min]`, ie., the last 10 features.\n",
        "<br><br>\n",
        "Next, we will do 1-D convolutions on `[batch_size, 240, 10]` dimensional inputs. Note that `240` here represents the input channels and you want to convolve over the context, ie., the `10` features.\n",
        "\n",
        "Finally, we will use a hidden test set to evaluate your model. In most real-world applications, you don't have access to your test data and its important to not overfit on your validation set. So make sure you do hyperparameter tuning in moderation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b29f9bee",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T05:46:21.980791Z",
          "start_time": "2022-02-23T05:46:21.973662Z"
        },
        "id": "b29f9bee"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8jIGLVTI6QW",
        "outputId": "b4928e82-d6c3-4371-9585-7a0c69a1eb32"
      },
      "id": "X8jIGLVTI6QW",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a3c32215",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T05:46:23.164379Z",
          "start_time": "2022-02-23T05:46:23.156046Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3c32215",
        "outputId": "85d1c2b1-8b5e-45eb-e752-5c032b3db657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "['souls' 'wheretheressmoke' 'thatthingonmyarm' 'hangtime' 'adollshouse'\n",
            " 'odetostepfather' 'sloth' 'myfirstdaywiththeyankees' 'buck' 'avatar'\n",
            " 'adventuresinsayingyes' 'exorcism' 'naked' 'haveyoumethimyet'\n",
            " 'stagefright' 'undertheinfluence' 'swimmingwithastronauts' 'itsabox'\n",
            " 'alternateithicatom' 'fromboyhoodtofatherhood']\n",
            "val\n",
            "['theclosetthatateeverything' 'tildeath' 'legacy']\n",
            "23\n"
          ]
        }
      ],
      "source": [
        "# Load the stories in the training and validation set.\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/HW2_phone_class_data/HW2_phone_class_data/data_split.pickle', 'rb') as f:\n",
        "    data_split_stories = pickle.load(f)\n",
        "for key in data_split_stories:\n",
        "    print(key)\n",
        "    print(data_split_stories[key])\n",
        "stories = np.concatenate([data_split_stories[key] for key in data_split_stories])\n",
        "print(len(stories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "eb2bd5a8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T05:46:26.051309Z",
          "start_time": "2022-02-23T05:46:23.773336Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb2bd5a8",
        "outputId": "3ce807f6-2682-450d-a5b5-970efac83d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "souls\n",
            "(72997, 240) (72997,)\n",
            "(6819,) (6819,) (6819,)\n",
            "wheretheressmoke\n",
            "(60191, 240) (60191,)\n",
            "(6068,) (6068,) (6068,)\n",
            "thatthingonmyarm\n",
            "(88875, 240) (88875,)\n",
            "(7080,) (7080,) (7080,)\n",
            "hangtime\n",
            "(66830, 240) (66830,)\n",
            "(6423,) (6423,) (6423,)\n",
            "adollshouse\n",
            "(50353, 240) (50353,)\n",
            "(5412,) (5412,) (5412,)\n",
            "odetostepfather\n",
            "(82808, 240) (82808,)\n",
            "(8334,) (8334,) (8334,)\n",
            "sloth\n",
            "(89497, 240) (89497,)\n",
            "(8595,) (8595,) (8595,)\n",
            "myfirstdaywiththeyankees\n",
            "(73686, 240) (73686,)\n",
            "(8866,) (8866,) (8866,)\n",
            "buck\n",
            "(68498, 240) (68498,)\n",
            "(5455,) (5455,) (5455,)\n",
            "avatar\n",
            "(75423, 240) (75423,)\n",
            "(5171,) (5171,) (5171,)\n",
            "adventuresinsayingyes\n",
            "(80356, 240) (80356,)\n",
            "(7850,) (7850,) (7850,)\n",
            "exorcism\n",
            "(95501, 240) (95501,)\n",
            "(9952,) (9952,) (9952,)\n",
            "naked\n",
            "(86502, 240) (86502,)\n",
            "(10281,) (10281,) (10281,)\n",
            "haveyoumethimyet\n",
            "(101320, 240) (101320,)\n",
            "(10176,) (10176,) (10176,)\n",
            "stagefright\n",
            "(60698, 240) (60698,)\n",
            "(6669,) (6669,) (6669,)\n",
            "undertheinfluence\n",
            "(62772, 240) (62772,)\n",
            "(5932,) (5932,) (5932,)\n",
            "swimmingwithastronauts\n",
            "(79121, 240) (79121,)\n",
            "(7318,) (7318,) (7318,)\n",
            "itsabox\n",
            "(73071, 240) (73071,)\n",
            "(5665,) (5665,) (5665,)\n",
            "alternateithicatom\n",
            "(70666, 240) (70666,)\n",
            "(7531,) (7531,) (7531,)\n",
            "fromboyhoodtofatherhood\n",
            "(71688, 240) (71688,)\n",
            "(8925,) (8925,) (8925,)\n",
            "theclosetthatateeverything\n",
            "(64932, 240) (64932,)\n",
            "(6566,) (6566,) (6566,)\n",
            "tildeath\n",
            "(66740, 240) (66740,)\n",
            "(7716,) (7716,) (7716,)\n",
            "legacy\n",
            "(81997, 240) (81997,)\n",
            "(6773,) (6773,) (6773,)\n"
          ]
        }
      ],
      "source": [
        "# For each story, load the pre-computed speech features and their associated timestamps.\n",
        "# Also load the phoneme transcriptions and the associated start/end times.\n",
        "speech_features, speech_timestamps, phonemes, phoneme_start_times, phoneme_end_times = [{} for _ in range(5)]\n",
        "for story in stories:\n",
        "    LOAD = np.load('/content/drive/My Drive/HW2_phone_class_data/HW2_phone_class_data/%s.npz'%story)\n",
        "    speech_features[story] = LOAD['fbank_features']\n",
        "    speech_timestamps[story] = LOAD['fbank_timestamps']\n",
        "    phonemes[story] = LOAD['phonemes']\n",
        "    phoneme_start_times[story] = LOAD['phoneme_start_times']\n",
        "    phoneme_end_times[story] = LOAD['phoneme_end_times']\n",
        "    print(story)\n",
        "    print(speech_features[story].shape, speech_timestamps[story].shape)\n",
        "    print(phonemes[story].shape, phoneme_start_times[story].shape, phoneme_end_times[story].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a4d78a",
      "metadata": {
        "id": "21a4d78a"
      },
      "source": [
        "### Q 2.1 Create classifier target dictionary (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9c68c352",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-22T22:54:30.956136Z",
          "start_time": "2022-02-22T22:54:30.947431Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c68c352",
        "outputId": "344677d3-75e4-4d9b-8de7-2e8cee42eb0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SIL' 'S' 'AY' 'EY' 'T' 'Y' 'ER' 'HH' 'SP' 'BR ' 'Z' 'AA' 'IY' 'OW' 'R'\n",
            " 'IH' 'BR' 'AO' 'DH' 'M' 'AH' 'B' 'K' 'L' 'W' 'EH' 'N' 'UW' 'OY' 'D' 'CH'\n",
            " 'F' 'NG' 'AE' 'AW' 'SH' 'P' 'TH' 'LG' 'LS' 'G' 'JH' 'V' 'UH' 'NS' 'CG' ''\n",
            " 'SPM' 'AHN' 'SP\\x7f' 'SPAH' 'ZH' '{NS}' '{IG}' '{CG}' 'ST' 'OA' 'OH' 'U'\n",
            " ' IY' ' AY' 'N ' 'A']\n",
            "Number of classes: 63\n"
          ]
        }
      ],
      "source": [
        "# Load the phoneme_classes which are the target of our CNN classifier.\n",
        "phoneme_classes = np.load('/content/drive/My Drive/HW2_phone_class_data/HW2_phone_class_data/phoneme_classes.npz')['arr_0']\n",
        "print(phoneme_classes)\n",
        "nclass = len(phoneme_classes)\n",
        "print('Number of classes:', nclass)\n",
        "# Create a dictionary mapping from the phoneme classes to an ID.\n",
        "# For example, if you were builiding a classifier of dog vs. cat,\n",
        "# this could be {'dog': 0, 'cat': 1} and [0, 1] would be your classifier targets.\n",
        "phone2int = {phone: i for i, phone in enumerate(phoneme_classes)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc5e94c5",
      "metadata": {
        "id": "dc5e94c5"
      },
      "source": [
        "### Q 2.2 Create features and labels by aligning timestamps (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "44745878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44745878",
        "outputId": "4b7fbe08-fbf8-447f-d8bc-bd707c372bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "souls\n",
            "(2331,) (2331, 10, 240)\n",
            "wheretheressmoke\n",
            "(1781,) (1781, 10, 240)\n",
            "thatthingonmyarm\n",
            "(3260,) (3260, 10, 240)\n",
            "hangtime\n",
            "(2327,) (2327, 10, 240)\n",
            "adollshouse\n",
            "(1136,) (1136, 10, 240)\n",
            "odetostepfather\n",
            "(2710,) (2710, 10, 240)\n",
            "sloth\n",
            "(2920,) (2920, 10, 240)\n",
            "myfirstdaywiththeyankees\n",
            "(2195,) (2195, 10, 240)\n",
            "buck\n",
            "(1993,) (1993, 10, 240)\n",
            "avatar\n",
            "(2372,) (2372, 10, 240)\n",
            "adventuresinsayingyes\n",
            "(2730,) (2730, 10, 240)\n",
            "exorcism\n",
            "(3210,) (3210, 10, 240)\n",
            "naked\n",
            "(2472,) (2472, 10, 240)\n",
            "haveyoumethimyet\n",
            "(2568,) (2568, 10, 240)\n",
            "stagefright\n",
            "(1899,) (1899, 10, 240)\n",
            "undertheinfluence\n",
            "(1643,) (1643, 10, 240)\n",
            "swimmingwithastronauts\n",
            "(2420,) (2420, 10, 240)\n",
            "itsabox\n",
            "(2106,) (2106, 10, 240)\n",
            "alternateithicatom\n",
            "(2360,) (2360, 10, 240)\n",
            "fromboyhoodtofatherhood\n",
            "(2330,) (2330, 10, 240)\n",
            "theclosetthatateeverything\n",
            "(2025,) (2025, 10, 240)\n",
            "tildeath\n",
            "(1985,) (1985, 10, 240)\n",
            "legacy\n",
            "(2248,) (2248, 10, 240)\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "window_size = 10\n",
        "features, labels = defaultdict(list), defaultdict(list)\n",
        "\n",
        "for story in stories:\n",
        "    print(story)\n",
        "    # For each phoneme in the story, store a list of speech timestamps `t`\n",
        "    # such that phoneme_start_time <= t <= phoneme_end_time.\n",
        "    # This is an example of what `phoneme_time_windows` could look like:\n",
        "    # [[0,1,2,5..10], [11,12,..27]...., [10000, 10001, ...10012]]\n",
        "    # In the above example, the first 10 speech features map to the first phoneme,\n",
        "    # features 11-27 match to second phoneme etc..\n",
        "    phoneme_time_windows = [[] for _ in range(len(phonemes[story]))]\n",
        "    for i in range(len(phonemes[story])):\n",
        "        indices = np.where((speech_timestamps[story] >= phoneme_start_times[story][i]) &\n",
        "                           (speech_timestamps[story] <= phoneme_end_times[story][i]))[0]\n",
        "        phoneme_time_windows[i] = indices.tolist()\n",
        "\n",
        "    for ix in range(len(phoneme_time_windows)):\n",
        "        phoneme_time_windows[ix] = np.array(phoneme_time_windows[ix])\n",
        "    # Compute and print the average number of speech features that match to 1 phoneme.\n",
        "    # For example, for a story there could on average be 13 features per phoneme.\n",
        "    # Store this value in `ptw_mean_length`. We will compute the difference between your\n",
        "    # variable and our pre-computed values to grade.\n",
        "    ptw_mean_length = np.mean([len(win) for win in phoneme_time_windows])\n",
        "    # Finally, we are going to select the 10 most recent features per phoneme\n",
        "    # and create our dataset.\n",
        "    for ix in range(len(phoneme_time_windows)):\n",
        "        # Discard any data points for which fewer than `window_size` speech features\n",
        "        # map onto the phoneme.\n",
        "        if phoneme_time_windows[ix].shape[0] < window_size:\n",
        "            continue\n",
        "        ph = phonemes[story][ix].upper().strip(\"0123456789\")\n",
        "        # Append the phoneme class ID.\n",
        "        labels[story].append(phone2int[ph])\n",
        "        # Append the `window_size` most recent features.\n",
        "        # You will have to make use of `phoneme_time_windows` of course to find the labels.\n",
        "        selected_indices = phoneme_time_windows[ix][-window_size:]\n",
        "        features[story].append(speech_features[story][selected_indices, :])\n",
        "    labels[story]= np.array(labels[story])\n",
        "    features[story] = np.array(features[story])\n",
        "    print(labels[story].shape, features[story].shape)\n",
        "\n",
        "    # Just making sure everything iss the right shape!\n",
        "    assert not np.any(np.isnan(features[story])), story\n",
        "    assert labels[story].shape[0] == features[story].shape[0]\n",
        "    assert np.all(features[story].shape[1:] == (window_size, 240))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6611688f",
      "metadata": {
        "id": "6611688f"
      },
      "source": [
        "### Q 2.3 Set up the dataloaders (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9dcbe33b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T00:59:57.063867Z",
          "start_time": "2022-02-23T00:59:57.045915Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dcbe33b",
        "outputId": "b315c8e6-33c1-44d0-f669-8e1308ab3431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(46763, 240, 10) (46763,)\n",
            "(6258, 240, 10) (6258,)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "\n",
        "# Stack all the speech features of the training set stories.\n",
        "# It is recommended to swap the 240-D and 10-D axes to make the convolution easier.\n",
        "train_feats = np.concatenate([\n",
        "    np.transpose(features[story], (0, 2, 1))  # (num_samples, 240, window_size)\n",
        "    for story in data_split_stories['train']\n",
        "], axis=0)\n",
        "# Stack all the phoneme labels of the training set stories.\n",
        "train_labels = np.concatenate([\n",
        "    labels[story]\n",
        "    for story in data_split_stories['train']\n",
        "], axis=0)\n",
        "print(train_feats.shape, train_labels.shape)\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_feats), torch.tensor(train_labels))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Repeat the process for the validation set.\n",
        "val_feats = np.concatenate([\n",
        "    np.transpose(features[story], (0, 2, 1))  # swap axes for convolution ease\n",
        "    for story in data_split_stories['val']\n",
        "], axis=0)\n",
        "val_labels = np.concatenate([\n",
        "    labels[story]\n",
        "    for story in data_split_stories['val']\n",
        "], axis=0)\n",
        "print(val_feats.shape, val_labels.shape)\n",
        "val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_feats), torch.tensor(val_labels))\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "### NOTE: It is super important that you double check your array shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678d2814",
      "metadata": {
        "id": "678d2814"
      },
      "source": [
        "### Q 2.4 Set up the training function. Note that this is identical to the FashionMNIST question (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "119266cb",
      "metadata": {
        "id": "119266cb"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_network(model, train_loader, val_loader, criterion, optimizer, nepoch=100):\n",
        "    try:\n",
        "        for epoch in tqdm(range(nepoch)):\n",
        "            print('EPOCH %d'%epoch)\n",
        "            total_loss = 0\n",
        "            count = 0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                count += 1\n",
        "            print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
        "            with torch.no_grad():\n",
        "                total_loss = 0\n",
        "                count = 0\n",
        "                for inputs, labels in val_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    total_loss += loss.item()\n",
        "                    count += 1\n",
        "                print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n",
        "            print()\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting from training early')\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c0f7b3e",
      "metadata": {
        "id": "8c0f7b3e"
      },
      "source": [
        "### Q 2.5 Define your CNN architecture. (3 points)\n",
        "\n",
        "- Make sure to use at least two 1-D convolution layers followed by at least one fully connected layers.\n",
        "- You will again find functions like `reshape`, `view`, `flatten` useful.\n",
        "- Your convolutions should treat the 240-D as _channels_ and combine information over the context, ie., the 10-D.\n",
        "- Build your model incrementally by checking the resultant output shape for each new layer you add! Test with an example batch.\n",
        "\n",
        "Documentation for Conv1D: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "448cb39c",
      "metadata": {
        "id": "448cb39c"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.nonlin = nn.ReLU()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=240, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(64 * 5, nclass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75eee90a",
      "metadata": {
        "id": "75eee90a"
      },
      "source": [
        "### Q 2.6 Define your optimizeer and criterion for the phoneme classifier. (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a48fb93b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48fb93b",
        "outputId": "88d74ad3-180c-4b77-c255-3cfe9ff20ba7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b99bc046770>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "model = CNNModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02da110",
      "metadata": {
        "id": "c02da110"
      },
      "source": [
        "### Q 2.7 How are weights and biases being initialized for the convolution layers? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169b42e8",
      "metadata": {
        "id": "169b42e8"
      },
      "source": [
        "PyTorch's convolutional layers use Kaiming uniform initialization for the weights, and the biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939c653b",
      "metadata": {
        "id": "939c653b"
      },
      "source": [
        "### Q 2.8 Train your network (5 pts)\n",
        "- Make sure you tune the hyperparameters!\n",
        "\n",
        "Train and validation losses should be both less than 1.5, and should be generally decreasing over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1751322d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ab1ff285e42444f2863869c78cf56e1e",
            "2a53709d6da94c758c41c3efe48c69ff",
            "ec3dc1c4541d40a6ba8352d2a0b899c2",
            "37eacb4b0b6a49b597a9a97cbf7a14e4",
            "82ac1343cbaf4fa7952896abff489699",
            "9a297b70bbb6404e8ba3f13142570c8d",
            "0510c5ce6e2f4c4ba6c3d28a778eb523",
            "86da7f01f0b94b29a8c3a3f65a7dcbd6",
            "34afe4c885754e3fb07947cd5abdb7e6",
            "eee9b57374854afcad3add769eafe1ff",
            "ea6759db29ab4bfead5ded49c591a74c"
          ]
        },
        "id": "1751322d",
        "outputId": "e3e72d68-7dd3-4c1e-f752-a511be7b4bd6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab1ff285e42444f2863869c78cf56e1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0\n",
            " Train loss: 2.36982\n",
            "   Val loss: 1.78263\n",
            "\n",
            "EPOCH 1\n",
            " Train loss: 1.47403\n",
            "   Val loss: 1.55743\n",
            "\n",
            "EPOCH 2\n",
            " Train loss: 1.28925\n",
            "   Val loss: 1.47795\n",
            "\n",
            "EPOCH 3\n",
            " Train loss: 1.19769\n",
            "   Val loss: 1.43538\n",
            "\n",
            "EPOCH 4\n",
            " Train loss: 1.12737\n",
            "   Val loss: 1.39290\n",
            "\n",
            "EPOCH 5\n",
            " Train loss: 1.07679\n",
            "   Val loss: 1.39670\n",
            "\n",
            "EPOCH 6\n",
            " Train loss: 1.03310\n",
            "   Val loss: 1.34932\n",
            "\n",
            "EPOCH 7\n",
            " Train loss: 1.00187\n",
            "   Val loss: 1.33673\n",
            "\n",
            "EPOCH 8\n",
            " Train loss: 0.96782\n",
            "   Val loss: 1.37011\n",
            "\n",
            "EPOCH 9\n",
            " Train loss: 0.93932\n",
            "   Val loss: 1.35597\n",
            "\n",
            "EPOCH 10\n",
            " Train loss: 0.91052\n",
            "   Val loss: 1.33204\n",
            "\n",
            "EPOCH 11\n",
            " Train loss: 0.87831\n",
            "   Val loss: 1.33093\n",
            "\n",
            "EPOCH 12\n",
            " Train loss: 0.85343\n",
            "   Val loss: 1.36710\n",
            "\n",
            "EPOCH 13\n",
            " Train loss: 0.83593\n",
            "   Val loss: 1.33905\n",
            "\n",
            "EPOCH 14\n",
            " Train loss: 0.81199\n",
            "   Val loss: 1.33575\n",
            "\n",
            "EPOCH 15\n",
            " Train loss: 0.78648\n",
            "   Val loss: 1.37302\n",
            "\n",
            "EPOCH 16\n",
            " Train loss: 0.76028\n",
            "   Val loss: 1.36490\n",
            "\n",
            "EPOCH 17\n",
            " Train loss: 0.74286\n",
            "   Val loss: 1.36056\n",
            "\n",
            "EPOCH 18\n",
            " Train loss: 0.72322\n",
            "   Val loss: 1.33951\n",
            "\n",
            "EPOCH 19\n",
            " Train loss: 0.70360\n",
            "   Val loss: 1.36801\n",
            "\n",
            "EPOCH 20\n",
            " Train loss: 0.74816\n",
            "   Val loss: 1.41630\n",
            "\n",
            "EPOCH 21\n",
            " Train loss: 0.69518\n",
            "   Val loss: 1.40432\n",
            "\n",
            "EPOCH 22\n",
            " Train loss: 0.65789\n",
            "   Val loss: 1.40045\n",
            "\n",
            "EPOCH 23\n",
            " Train loss: 0.63754\n",
            "   Val loss: 1.42184\n",
            "\n",
            "EPOCH 24\n",
            " Train loss: 0.61741\n",
            "   Val loss: 1.43054\n",
            "\n",
            "EPOCH 25\n",
            " Train loss: 0.59756\n",
            "   Val loss: 1.48108\n",
            "\n",
            "EPOCH 26\n",
            " Train loss: 0.57927\n",
            "   Val loss: 1.47664\n",
            "\n",
            "EPOCH 27\n",
            " Train loss: 0.56362\n",
            "   Val loss: 1.46978\n",
            "\n",
            "EPOCH 28\n",
            " Train loss: 0.55370\n",
            "   Val loss: 1.48215\n",
            "\n",
            "EPOCH 29\n",
            " Train loss: 0.52413\n",
            "   Val loss: 1.53039\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = CNNModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "train_network(model, train_loader, val_loader, criterion, optimizer, nepoch=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e7fce87",
      "metadata": {
        "id": "3e7fce87"
      },
      "source": [
        "### Q 2.9 Set up the test function. Note that this is identical to the FashionMNIST question (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b263a41e",
      "metadata": {
        "id": "b263a41e"
      },
      "outputs": [],
      "source": [
        "def test_network(model, test_loader, mode):\n",
        "    true, pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels  in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "            true.append(labels)\n",
        "            pred.append(predicted)\n",
        "    acc = (np.concatenate(true) == np.concatenate(pred)).mean()\n",
        "    print('%s accuracy: %0.3f' % (mode, acc))\n",
        "    true = np.concatenate(true)\n",
        "    pred = np.concatenate(pred)\n",
        "    return acc, true, pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e03b9858",
      "metadata": {
        "id": "e03b9858"
      },
      "source": [
        "### Q 2.10 Write a function that takes in the model predictions and true labels, and returns the accuracy per phoneme class as a dictionary (3 pts)\n",
        "\n",
        "For example, if the true labels contain 10 instances of the phoneme class \"AY\" and the model predicts this correctly 5 times, the entry `class_accuracy['AY'] = 0.5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "0a1a2091",
      "metadata": {
        "id": "0a1a2091"
      },
      "outputs": [],
      "source": [
        "def report_test_results(precomputed_acc, model_true, model_pred):\n",
        "    # Accuracy per phoneme class.\n",
        "    class_accuracy = np.zeros(nclass)\n",
        "    for i in range(nclass):\n",
        "        total_i = np.sum(model_true == i)\n",
        "        class_accuracy[i] = np.sum((model_true == i) & (model_pred == i)) / total_i if total_i > 0 else 0\n",
        "    # Write an assertion to check that the sum of correctly predicted\n",
        "    # labels across all classes is equal to the total accuracy of the model.\n",
        "    # It is good (and important) to always run such sanity checks!\n",
        "    # PS. You will probably have to define more variables.\n",
        "    assert np.allclose(np.sum(model_true == model_pred) / len(model_true), precomputed_acc)\n",
        "    # Sort the phoneme classes in increasing order of accuracy.\n",
        "    sorted_phones = np.array(phoneme_classes)[np.argsort(class_accuracy)]\n",
        "    print('Top-3 predicted phones:', sorted_phones[-3:])\n",
        "    return class_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85d5f56",
      "metadata": {
        "id": "b85d5f56"
      },
      "source": [
        "### Q 2.11 Run tests on the validation data (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0e1028b7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T06:16:00.843445Z",
          "start_time": "2022-02-23T06:16:00.828134Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e1028b7",
        "outputId": "1a05606b-e063-4c16-e30a-51ded0147010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.583\n",
            "Top-3 predicted phones: ['AY' 'K' 'SP']\n"
          ]
        }
      ],
      "source": [
        "val_precomputed_acc, val_model_true, val_model_pred = test_network(model, val_loader, 'Validation')\n",
        "val_class_accuracy = report_test_results(\n",
        "    val_precomputed_acc, val_model_true, val_model_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6601e7f5",
      "metadata": {
        "id": "6601e7f5"
      },
      "source": [
        "### Q 2.12 Print the validation class accuracy for the given phoneme classes. What do you observe? What could be the reason for this? (2 pts)\n",
        "\n",
        "HINT: The answer is UNRELATED to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7dfa7cab",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-22T23:36:02.196554Z",
          "start_time": "2022-02-22T23:36:02.190833Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dfa7cab",
        "outputId": "034b2432-2e65-4352-f9e1-0908800774db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OH 0.0\n",
            "U 0.0\n"
          ]
        }
      ],
      "source": [
        "for phoneme in ['OH', 'U']:\n",
        "    idx = np.where(phoneme_classes == phoneme)[0][0]\n",
        "    print(phoneme, val_class_accuracy[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10abd4a5",
      "metadata": {
        "id": "10abd4a5"
      },
      "source": [
        "### Q 2.13 We will now test your model on a hidden test set. So make sure your model definition and functions to compute test accuracy are clearly defined! (2 pts)\n",
        "\n",
        "_(there is nothing for you to do here, this block exists for grading purposes only)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1208dc11",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-23T06:20:39.863685Z",
          "start_time": "2022-02-23T06:20:35.733361Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "1208dc11",
        "outputId": "be9e3a10-1233-424b-f392-c750e9927bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inamoment\n",
            "(43031, 240) (43031,)\n",
            "(3494,) (3494,) (3494,)\n",
            "eyespy\n",
            "(77941, 240) (77941,)\n",
            "(7920,) (7920,) (7920,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-30163168cccd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_split_stories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mLOAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/HW2_phone_class_data/HW2_phone_class_data/%s.npz'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mspeech_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOAD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fbank_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mspeech_timestamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOAD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fbank_timestamps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mphonemes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOAD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phonemes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 return format.read_array(bytes,\n\u001b[0m\u001b[1;32m    257\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    860\u001b[0m                                                              count=read_count)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_STORED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    784\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data_split_stories['test'] = ['inamoment', 'eyespy', 'life', 'howtodraw']\n",
        "\n",
        "for story in data_split_stories['test']:\n",
        "    LOAD = np.load('/content/drive/My Drive/HW2_phone_class_data/HW2_phone_class_data/%s.npz'%story)\n",
        "    speech_features[story] = LOAD['fbank_features']\n",
        "    speech_timestamps[story] = LOAD['fbank_timestamps']\n",
        "    phonemes[story] = LOAD['phonemes']\n",
        "    phoneme_start_times[story] = LOAD['phoneme_start_times']\n",
        "    phoneme_end_times[story] = LOAD['phoneme_end_times']\n",
        "    print(story)\n",
        "    print(speech_features[story].shape, speech_timestamps[story].shape)\n",
        "    print(phonemes[story].shape, phoneme_start_times[story].shape, phoneme_end_times[story].shape)\n",
        "\n",
        "features, labels = defaultdict(list), defaultdict(list)\n",
        "\n",
        "for story in data_split_stories['test']:\n",
        "    print(story)\n",
        "    phoneme_time_windows = [[] for _ in range(len(phonemes[story]))]\n",
        "    for time_index, time in enumerate(speech_timestamps[story]):\n",
        "        # Find position which starts at or right before this timestamp.\n",
        "        ix = np.where(phoneme_start_times[story] <= time)[0][-1]\n",
        "        if phoneme_end_times[story][ix] > time:\n",
        "            phoneme_time_windows[ix].append(time_index)\n",
        "    for ix in range(len(phoneme_time_windows)):\n",
        "        phoneme_time_windows[ix] = np.array(phoneme_time_windows[ix])\n",
        "    ptw_lengths = np.array([len(ptw) for ptw in phoneme_time_windows])\n",
        "    print(ptw_lengths.mean(), ptw_lengths.std())\n",
        "    for ix in range(len(phoneme_time_windows)):\n",
        "        if len(phoneme_time_windows[ix]) < window_size:\n",
        "            continue\n",
        "        ph = phonemes[story][ix].upper().strip(\"0123456789\")\n",
        "        labels[story].append(phone2int[ph])\n",
        "        features[story].append(speech_features[story][phoneme_time_windows[ix]][-window_size:])\n",
        "    labels[story]= np.array(labels[story])\n",
        "    features[story] = np.array(features[story])\n",
        "    print(labels[story].shape, features[story].shape)\n",
        "\n",
        "    assert not np.any(np.isnan(features[story])), story\n",
        "    assert labels[story].shape[0] == features[story].shape[0]\n",
        "    assert np.all(features[story].shape[1:] == (window_size, 240))\n",
        "\n",
        "# Set up the dataloaders\n",
        "test_feats = np.vstack([features[story] for story in data_split_stories['test']])\n",
        "test_feats = np.swapaxes(test_feats, 1, 2)\n",
        "test_labels = np.hstack([labels[story] for story in data_split_stories['test']])\n",
        "print(test_feats.shape, test_labels.shape)\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_feats), torch.tensor(test_labels))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "precomputed_acc, model_true, model_pred = test_network(model, test_loader, 'test')\n",
        "report_test_results(precomputed_acc, model_true, model_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67fb78b",
      "metadata": {
        "id": "d67fb78b"
      },
      "source": [
        "### Q 2.14 Can you reuse your model as is with a different `window_size`? Why or why not? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c00e9bc",
      "metadata": {
        "id": "1c00e9bc"
      },
      "source": [
        "No, the model is built for a fixed window_size, so its convolutional and fully-connected layers expect specific dimensions. Changing the window_size alters these dimensions, requiring architectural adjustments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aab750e",
      "metadata": {
        "id": "6aab750e"
      },
      "source": [
        "### Q 2.15 Can you reuse your model as is with a different dimensionality of speech features? Why or why not? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04f5e2b",
      "metadata": {
        "id": "a04f5e2b"
      },
      "source": [
        "No, the model’s first convolution layer is fixed to the original feature dimensionality, so changing it will cause a mismatch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa22810",
      "metadata": {
        "id": "afa22810"
      },
      "source": [
        "## Problem 3. 1-D CNNs and model interpretation (27 pts)\n",
        "\n",
        "In this problem you will again train a 1-D CNN on audio data, here to classify spoken numerals (e.g. \"one\", \"two\"). Then you will use additional techniques to interpret (since it's audio, one can't really say \"visualize\", but same idea) what the model is doing. Run these cells first to set things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "de7a3241",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "de7a3241",
        "outputId": "ad649b26-9251-4b69-c55c-4207328c0ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:39<00:00, 61.2MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-169db3d14437>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maudio_save_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;31m#SPEECHCOMMANDS_data' # set to wherever you want to keep these files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudiodatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPEECHCOMMANDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msc_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudiodatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPEECHCOMMANDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msc_testing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudiodatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPEECHCOMMANDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/datasets/speechcommands.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, url, folder_in_archive, download, subset)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mchecksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CHECKSUMS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0m_extract_tar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/datasets/utils.py\u001b[0m in \u001b[0;36m_extract_tar\u001b[0;34m(from_path, to_path, overwrite)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner, filter)\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_extract_tarinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_owner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_extract_tarinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36m_extract_one\u001b[0;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[0m\u001b[1;32m   2358\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m                                  numeric_owner=numeric_owner)\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2492\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmakeunknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio import datasets as audiodatasets\n",
        "\n",
        "audio_save_dir = './'#SPEECHCOMMANDS_data' # set to wherever you want to keep these files\n",
        "sc_training = audiodatasets.SPEECHCOMMANDS(audio_save_dir, download=True, subset=\"training\")\n",
        "sc_validation = audiodatasets.SPEECHCOMMANDS(audio_save_dir, download=True, subset=\"validation\")\n",
        "sc_testing = audiodatasets.SPEECHCOMMANDS(audio_save_dir, download=True, subset=\"testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d2d4bc8c",
      "metadata": {
        "id": "d2d4bc8c"
      },
      "outputs": [],
      "source": [
        "# the audio will be downsampled to 8 kHz to make it easier to work with. just run this\n",
        "new_sample_rate = 8000\n",
        "transform = torchaudio.transforms.Resample(orig_freq=16000, new_freq=new_sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "81578c87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "81578c87",
        "outputId": "06581270-7773-4ff9-da1b-5b3b256180b7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc_training' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-b6bd22948920>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msel_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"zero\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"one\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"two\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"three\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"four\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"five\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"six\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seven\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nine\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc_training\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msel_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc_validation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msel_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtesting_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc_testing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msel_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc_training' is not defined"
          ]
        }
      ],
      "source": [
        "# the dataset contains a lot of words, but let's focus on the numbers\n",
        "sel_labels = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
        "\n",
        "training_inds = np.array([ii for ii,datum in enumerate(sc_training) if datum[2] in sel_labels])\n",
        "validation_inds = np.array([ii for ii,datum in enumerate(sc_validation) if datum[2] in sel_labels])\n",
        "testing_inds = np.array([ii for ii,datum in enumerate(sc_testing) if datum[2] in sel_labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be2b0d2f",
      "metadata": {
        "id": "be2b0d2f"
      },
      "outputs": [],
      "source": [
        "# define function that will pad each sample in a batch to the same length\n",
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360af77d",
      "metadata": {
        "id": "360af77d"
      },
      "outputs": [],
      "source": [
        "# make data loaders\n",
        "\n",
        "def collate_fn(batch):\n",
        "    tensors, targets = [], []\n",
        "    for waveform, _, label, *_ in batch:\n",
        "        tensors += [waveform]\n",
        "        targets += [torch.Tensor([sel_labels.index(label)]).squeeze().long()]\n",
        "\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    sc_training,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    sampler=SubsetRandomSampler(training_inds)\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    sc_validation,\n",
        "    batch_size=batch_size,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    sampler=SubsetRandomSampler(validation_inds)\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    sc_testing,\n",
        "    batch_size=batch_size,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    sampler=SubsetRandomSampler(testing_inds)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d32942",
      "metadata": {
        "id": "70d32942"
      },
      "source": [
        "### Q 3.1: Plot the waveform and play the audio for one training sample (2 pts)\n",
        "Set the x-axis values so that they show seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55edccff",
      "metadata": {
        "id": "55edccff"
      },
      "outputs": [],
      "source": [
        "# plot the waveform for one example (e.g., sc_training[150])\n",
        "\n",
        "plt.figure(figsize=(5,2))\n",
        "plt.plot(time, waveform[0].numpy())\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# play the audio\n",
        "ipd.Audio(waveform.numpy(),, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bbbc3eb",
      "metadata": {
        "id": "8bbbc3eb"
      },
      "source": [
        "### Q 3.2: Define model (3 pts)\n",
        "Assume you are given the basic number of channels, `n_channels`. This model should have the following layers:\n",
        "* 1-D convolutional layer with `n_channels` kernels of length 80, with a stride of 16\n",
        "* 1-D batch norm layer\n",
        "* ReLU\n",
        "* 1-D max pooling layer with kernel size 4, stride 4\n",
        "* 1-D convolutional layer with `n_channels` kernels of size 3, with a stride of 1\n",
        "* 1-D batch norm layer\n",
        "* ReLU\n",
        "* 1-D max pooling layer with same parameters as above\n",
        "* 1-D convolutional layer with `2*n_channels` kernels of size 3, with a stride of 1\n",
        "* 1-D batch norm layer\n",
        "* ReLU\n",
        "* 1-D max pooling with same parameters as above\n",
        "* 1-D convolutional layer with `2*n_channels` kernels of size 3, with a stride of 1\n",
        "* 1-D batch norm layer\n",
        "* ReLU\n",
        "* 1-D max pooling with same parameters as above\n",
        "* average pooling across all remaining timepoints (see `AdaptiveAvgPool1d`)\n",
        "* flattening\n",
        "* Linear layer with `2*n_channels` inputs and 10 outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c04f75",
      "metadata": {
        "id": "11c04f75"
      },
      "outputs": [],
      "source": [
        "class WordRecognizer(nn.Module):\n",
        "    def __init__(self, n_input=1, n_output=10, stride=16, n_channel=32):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=n_input, out_channels=n_channel, kernel_size=80, stride=stride),\n",
        "            nn.BatchNorm1d(n_channel),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "            nn.Conv1d(in_channels=n_channel, out_channels=n_channel, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm1d(n_channel),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "            nn.Conv1d(in_channels=n_channel, out_channels=2*n_channel, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm1d(2*n_channel),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "            nn.Conv1d(in_channels=2*n_channel, out_channels=2*n_channel, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm1d(2*n_channel),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
        "        )\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(2*n_channel, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = WordRecognizer(n_input=transformed.shape[0], n_output=len(sel_labels), n_channel=32)\n",
        "print(model)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n = count_parameters(model)\n",
        "print(\"Number of parameters: %s\" % n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6902c9",
      "metadata": {
        "id": "da6902c9"
      },
      "source": [
        "### Q 3.3: Write the training and test functions (5 pts)\n",
        "Remember you need to transform the data (using the `transform` function) before you put it into the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f462ce",
      "metadata": {
        "id": "99f462ce"
      },
      "outputs": [],
      "source": [
        "# Run this cell first\n",
        "\n",
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "# let's use StepLR to reduce the learning after 20 epochs by a factor of 10\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "lossfunction = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5399646e",
      "metadata": {
        "id": "5399646e"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "# write a function that does one training epoch\n",
        "def train_one_epoch(model):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = transform(inputs)\n",
        "        outputs = model(inputs)\n",
        "        loss = lossfunction(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = get_likely_index(outputs)\n",
        "        total_correct += number_of_correct(pred, targets)\n",
        "        total_samples += targets.size(0)\n",
        "    scheduler.step()\n",
        "    print(f\"Train Loss: {total_loss/len(train_loader):.3f}, Train Accuracy: {total_correct/total_samples:.3f}\")\n",
        "\n",
        "# write a function that computes & prints test accuracy\n",
        "# (hint: use the `number_of_correct` and `get_likely_index` functions)\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            # Transform the data before passing it to the model\n",
        "            inputs = transform(inputs)\n",
        "            outputs = model(inputs)\n",
        "            loss = lossfunction(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            pred = get_likely_index(outputs)\n",
        "            total_correct += number_of_correct(pred, targets)\n",
        "            total_samples += targets.size(0)\n",
        "    test_acc = total_correct/total_samples\n",
        "    print(\"Test Loss: {:.3f}, Test Accuracy: {:.3f}\".format(total_loss/len(test_loader), test_acc))\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9934c7fe",
      "metadata": {
        "id": "9934c7fe"
      },
      "source": [
        "### Q 3.4: Fit the model! Tweak until you get test accuracy of at least 85% (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc794fc5",
      "metadata": {
        "id": "bc794fc5"
      },
      "outputs": [],
      "source": [
        "n_epoch = 40\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    train(model)\n",
        "    test(model)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cc4a0b",
      "metadata": {
        "id": "43cc4a0b"
      },
      "source": [
        "### Q 3.5: Create contingency matrix to show performance (2 pts)\n",
        "`contingency` should be a 10x10 matrix where each row is a predicted label and each column a true label. The value of each element in this matrix should be the number of times a test example with the true label given by the column was assigned the predicted label given by the row. For example, if the predicted label for an example was \"seven\" but the true label was \"six\", then you should add one to `contingency[7,6]`. Do this for all examples in the test set.\n",
        "\n",
        "This is a nice way to visualize how well the model is working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe1afca",
      "metadata": {
        "id": "dbe1afca"
      },
      "outputs": [],
      "source": [
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "# get predictions for each test example. you can use same methods as `test` function, above\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = transform(inputs)\n",
        "        outputs = model(inputs)\n",
        "        pred = get_likely_index(outputs)\n",
        "        all_preds.extend(pred.tolist())\n",
        "        all_targets.extend(targets.tolist())\n",
        "\n",
        "# create contingency table\n",
        "#\n",
        "contingency = np.zeros((10, 10))\n",
        "for p, t in zip(all_preds, all_targets):\n",
        "    contingency[p, t] += 1\n",
        "\n",
        "plt.matshow(contingency)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c37edde",
      "metadata": {
        "id": "5c37edde"
      },
      "source": [
        "### Q 3.6: Create a new model and use it to generate optimized inputs (10 pts)\n",
        "Finally, let's try to generate new sounds that would maximally activate each of the output units in our `WordRecognizer` model. Input optimization is one type of feature visualization (in this case, \"audiolization\") that can help understand how a model works.\n",
        "\n",
        "We will do this by using gradient backpropagation to generate sounds that maximally activate each of the `WordRecognizer` outputs. This requires us to define a new model, `InputOptim`, which has one parameter tensor: `optimized_input`, the input that it is trying to optimize. In its `forward` method, this model should apply the pretrained `WordRecognizer` to its `optimized_input` and return the result. We will then use some optimizer (e.g. Adam) to make the `WordRecognizer` output the desired value. We will repeat this for each of the output classes (\"zero\", \"one\", \"two\", etc.). Then we will check to make sure that this works correctly (after optimization, the `WordRecognizer` should be 100% certain that the optimized input belongs to the desired class) and listen to the resulting sounds.\n",
        "\n",
        "There are two things that you should consider here:\n",
        "* How should you initialize `optimized_input`? There are many possibilities, and this choice will greatly affect your result.\n",
        "* How should you do the optimization? Number of epochs, weight decay, and other optimization choices will also have a big effect on your result.\n",
        "\n",
        "At the end of the problem you will be asked to write down some observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6d27c0",
      "metadata": {
        "id": "ae6d27c0"
      },
      "outputs": [],
      "source": [
        "# first, define the new model\n",
        "# this model should have one parameter tensor: the input vector you are optimization\n",
        "# its `forward` function should apply the pretrained `WordRecognizer` model to this model's `optimized_input`\n",
        "\n",
        "class InputOptim(nn.Module):\n",
        "    def __init__(self, recognizer_model, input_shape=(1,1,8000)):\n",
        "        super().__init__()\n",
        "        self.recognizer_model = recognizer_model\n",
        "        self.optimized_input = nn.Parameter(torch.randn(input_shape)) # initialize parameter tensor, should have requires_grad=True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.recognizer_model(self.optimized_input)\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.optimized_input]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afafbcd7",
      "metadata": {
        "id": "afafbcd7"
      },
      "outputs": [],
      "source": [
        "# next, train 10 `InputOptim` models, one for each output class (\"zero\", \"one\", etc.)\n",
        "targets = torch.arange(10).long()\n",
        "opt_stims = []\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for t in targets:\n",
        "    input_model = InputOptim(recognizer_model=model, input_shape=(1,1,8000))\n",
        "    optimizer = torch.optim.Adam(input_model.parameters(), lr=0.1, weight_decay=1e-4) # make sure this only optimizes the `input_model` parameters!\n",
        "    lossfxn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"target: \", t)\n",
        "    for epoch in range(n_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = input_model(None)\n",
        "        target_tensor = torch.tensor([t])\n",
        "        loss = lossfxn(output, target_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.3f}\")\n",
        "\n",
        "    opt_stims.append(input_model.optimized_input.detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d742c2c8",
      "metadata": {
        "id": "d742c2c8"
      },
      "outputs": [],
      "source": [
        "# finally, use this cell to see model predictions for the optimized inputs & listen to the sounds\n",
        "\n",
        "def predict(tensor):\n",
        "    # Use the model to predict the label of the waveform\n",
        "    logits = model(tensor.unsqueeze(0))\n",
        "    tensor = get_likely_index(logits)\n",
        "    tensor = sel_labels[tensor.squeeze().item()]\n",
        "    return logits.squeeze().detach().numpy(), tensor\n",
        "\n",
        "for ind in range(10):\n",
        "    utterance = sel_labels[ind]\n",
        "\n",
        "    probs, pred = predict(torch.Tensor(opt_stims[ind].reshape(1,8000)))\n",
        "    print(f\"Expected: {utterance}. Predicted: {pred}.\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.bar(range(10), np.exp(probs) / np.exp(probs).sum())\n",
        "    plt.xticks(range(10), sel_labels)\n",
        "\n",
        "    ipd.Audio(opt_stims[ind].squeeze(), rate=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c19c2d",
      "metadata": {
        "id": "d3c19c2d"
      },
      "source": [
        "### Q 3.7: What happens if you initialize `InputOptim` using a real sound, such as one of the training examples? What does this tell us about how this model works? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63e01ef",
      "metadata": {
        "id": "c63e01ef"
      },
      "source": [
        "Initializing InputOptim with a real sound leads the optimization to refine the input while retaining recognizable acoustic features. This shows that the model's learned representation effectively captures real audio characteristics and is sensitive to meaningful input patterns."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "222.86685180664062px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e769976b4b84f7f8a97894d7cc3657d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e42811b0fe84da08b7857da66660cd9",
              "IPY_MODEL_aaa6247976314d56b90fcdd602d5d92f",
              "IPY_MODEL_fc1a191696054d6982db7026bfbc64b4"
            ],
            "layout": "IPY_MODEL_5a03fc8fe49c40cf995b3f0293573e4e"
          }
        },
        "8e42811b0fe84da08b7857da66660cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7bea91d05f8419eacfb411f1fff1190",
            "placeholder": "​",
            "style": "IPY_MODEL_9cab7259f6a549d3870f5918b9a5dbf3",
            "value": "  0%"
          }
        },
        "aaa6247976314d56b90fcdd602d5d92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4edc42089643408280a6f7a752e07dcf",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed6732a7917d452195e770407e636d74",
            "value": 0
          }
        },
        "fc1a191696054d6982db7026bfbc64b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9b86d8cab564bd9b3de1239370e6b2f",
            "placeholder": "​",
            "style": "IPY_MODEL_b760c414aa554eaab3d43c0727088b50",
            "value": " 0/20 [00:02&lt;?, ?it/s]"
          }
        },
        "5a03fc8fe49c40cf995b3f0293573e4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7bea91d05f8419eacfb411f1fff1190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cab7259f6a549d3870f5918b9a5dbf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4edc42089643408280a6f7a752e07dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6732a7917d452195e770407e636d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9b86d8cab564bd9b3de1239370e6b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b760c414aa554eaab3d43c0727088b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1c7290d23e6462eb4a63fefbe2b15b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89d10523ce6547d4bcc6b92f05fd6f30",
              "IPY_MODEL_511651f9142344279ad45b72a951cd76",
              "IPY_MODEL_a09d5601f7184bb9adcb3632f2482c19"
            ],
            "layout": "IPY_MODEL_5ac512bb923b477e9b442b77586d9de9"
          }
        },
        "89d10523ce6547d4bcc6b92f05fd6f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a96b9ada84949758fa13f7bccf5d9a8",
            "placeholder": "​",
            "style": "IPY_MODEL_422e407e37fd433b8d4cd2dec5b431c8",
            "value": "100%"
          }
        },
        "511651f9142344279ad45b72a951cd76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeea291271484fd38b61a860789e91f4",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66a7648bf8e84675a1f8216864424672",
            "value": 20
          }
        },
        "a09d5601f7184bb9adcb3632f2482c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e203eaa3b71c463fa0ca2e11ba5705a9",
            "placeholder": "​",
            "style": "IPY_MODEL_8eba5cba374846258d250c9b6739b4f1",
            "value": " 20/20 [44:54&lt;00:00, 135.54s/it]"
          }
        },
        "5ac512bb923b477e9b442b77586d9de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a96b9ada84949758fa13f7bccf5d9a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "422e407e37fd433b8d4cd2dec5b431c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeea291271484fd38b61a860789e91f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a7648bf8e84675a1f8216864424672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e203eaa3b71c463fa0ca2e11ba5705a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eba5cba374846258d250c9b6739b4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab1ff285e42444f2863869c78cf56e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a53709d6da94c758c41c3efe48c69ff",
              "IPY_MODEL_ec3dc1c4541d40a6ba8352d2a0b899c2",
              "IPY_MODEL_37eacb4b0b6a49b597a9a97cbf7a14e4"
            ],
            "layout": "IPY_MODEL_82ac1343cbaf4fa7952896abff489699"
          }
        },
        "2a53709d6da94c758c41c3efe48c69ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a297b70bbb6404e8ba3f13142570c8d",
            "placeholder": "​",
            "style": "IPY_MODEL_0510c5ce6e2f4c4ba6c3d28a778eb523",
            "value": "100%"
          }
        },
        "ec3dc1c4541d40a6ba8352d2a0b899c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86da7f01f0b94b29a8c3a3f65a7dcbd6",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34afe4c885754e3fb07947cd5abdb7e6",
            "value": 30
          }
        },
        "37eacb4b0b6a49b597a9a97cbf7a14e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eee9b57374854afcad3add769eafe1ff",
            "placeholder": "​",
            "style": "IPY_MODEL_ea6759db29ab4bfead5ded49c591a74c",
            "value": " 30/30 [05:49&lt;00:00, 11.42s/it]"
          }
        },
        "82ac1343cbaf4fa7952896abff489699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a297b70bbb6404e8ba3f13142570c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0510c5ce6e2f4c4ba6c3d28a778eb523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86da7f01f0b94b29a8c3a3f65a7dcbd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34afe4c885754e3fb07947cd5abdb7e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eee9b57374854afcad3add769eafe1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea6759db29ab4bfead5ded49c591a74c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}